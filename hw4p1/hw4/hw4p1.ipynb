{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSLkT0qL3jgl"
   },
   "source": [
    "# HW4P1: Language Modelling\n",
    "\n",
    "Welcome to the final part 1 hw of this course. This is the only part 1 in which you have PyTorch training (Yay). You will be working on training language models and evaluating them on the task of prediction and generation.<br>\n",
    "The model which you will be coding in this HW very similar to the Speller module from HW4P2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EB2bOV3bzYLR"
   },
   "source": [
    "# Get modules and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r4_-qG9rSULt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummaryX==1.1.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/pytorch/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: seaborn in /opt/conda/envs/pytorch/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (21.3)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/envs/pytorch/lib/python3.11/site-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from packaging>=20.0->transformers) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: tokenizers in /opt/conda/envs/pytorch/lib/python3.11/site-packages (0.21.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from tokenizers) (0.26.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummaryX==1.1.0\n",
    "!pip install wandb --quiet\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "\n",
    "!pip install transformers -U\n",
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNTfP7QDYkOP"
   },
   "outputs": [],
   "source": [
    "# # TODO: Import drive if you are using Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QZNwme4320LW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/home/ubuntu/IDL/hw4p1/dataset/'\n",
      "/home/ubuntu/IDL/hw4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "path = '/home/ubuntu/IDL/hw4p1/dataset/' # TODO: Add path to handout. For example ~/IDL/hw4/hw4p1_handout\n",
    "sys.path.append(path)\n",
    "%cd {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INh9p3v3zbF_"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oxiZ42B4SwQ-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import torchsummaryX\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import gc\n",
    "import glob\n",
    "import wandb\n",
    "import yaml\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: \", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loUYgkv8moMl"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ybSqv89zmoMm"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"token_type\":  'char', # TODO: select a tokenzier from [\"char\", \"1k\", \"10k\"]\n",
    "    \"d_model\":     512,\n",
    "    \"num_layers\":  6,\n",
    "    \"num_heads\":   8,\n",
    "    \"d_ff\":        2048,\n",
    "    \"dropout\":     0.1,\n",
    "    \"max_length\":  256,\n",
    "    \"lr\":          1e-4,\n",
    "    \"batch_size\":  32,\n",
    "    \"num_epochs\":  10,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdG352aFYdyz"
   },
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9D39sk7AYdy0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary    : 31\n",
      "VOCAB                   : ['<sos>', '<eos>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \"'\", ' ', '<pad>']\n",
      "PAD_TOKEN               : 30\n",
      "SOS_TOKEN               : 0\n",
      "EOS_TOKEN               : 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>transcripts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>CHAPTER TEN SHAGGY MAN TO THE RESCUE THEY HAD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AT ONCE THEY HURRIED FORWARD TO SEE WHAT THIS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AND PICKED OUT THE EASIEST PLACES TO GO ALL IT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>LEAVING HOLES THAT MIGHT CAUSE THE UNWARY TO S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>WITH MY HEART RENDING GROWL MY HORRIBLE SHUDDE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        transcripts\n",
       "0           0  CHAPTER TEN SHAGGY MAN TO THE RESCUE THEY HAD ...\n",
       "1           1  AT ONCE THEY HURRIED FORWARD TO SEE WHAT THIS ...\n",
       "2           2  AND PICKED OUT THE EASIEST PLACES TO GO ALL IT...\n",
       "3           3  LEAVING HOLES THAT MIGHT CAUSE THE UNWARY TO S...\n",
       "4           4  WITH MY HEART RENDING GROWL MY HORRIBLE SHUDDE..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the vocabulary. Try printing and see\n",
    "VOCAB = [\n",
    "   \"<sos>\", \"<eos>\",\n",
    "    \"A\",   \"B\",    \"C\",    \"D\",\n",
    "    \"E\",   \"F\",    \"G\",    \"H\",\n",
    "    \"I\",   \"J\",    \"K\",    \"L\",\n",
    "    \"M\",   \"N\",    \"O\",    \"P\",\n",
    "    \"Q\",   \"R\",    \"S\",    \"T\",\n",
    "    \"U\",   \"V\",    \"W\",    \"X\",\n",
    "    \"Y\",   \"Z\",    \"'\",    \" \", \"<pad>\"\n",
    "]\n",
    "\n",
    "VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n",
    "# We have also included <sos> and <eos> in the vocabulary for you\n",
    "# However in real life, you include it explicitly if not provided\n",
    "PAD_TOKEN =  VOCAB_MAP[\"<pad>\"]\n",
    "SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n",
    "EOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n",
    "\n",
    "print(f\"Length of Vocabulary    : {len(VOCAB)}\")\n",
    "print(f\"VOCAB                   : {VOCAB}\")\n",
    "print(f\"PAD_TOKEN               : {PAD_TOKEN}\")\n",
    "print(f\"SOS_TOKEN               : {SOS_TOKEN}\")\n",
    "print(f\"EOS_TOKEN               : {EOS_TOKEN}\")\n",
    "\n",
    "df_train = pd.read_csv(\"/home/ubuntu/IDL/dataset/train-clean-100/transcripts.csv\")\n",
    "df_val = pd.read_csv(\"/home/ubuntu/IDL/dataset/dev-clean/transcripts.csv\")\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9XKR8hhmFslQ"
   },
   "outputs": [],
   "source": [
    "class CharTokenizer():\n",
    "    ''' A wrapper around character tokenization to have a consistent interface with other tokeization strategies'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.eos_token = \"<|endoftext|>\"  # Same as EOS_TOKEN\n",
    "        self.pad_token = \"<|padding|>\"\n",
    "        self.unk_token = \"<|unknown|>\"\n",
    "\n",
    "        characters = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ '\")\n",
    "\n",
    "        # Create vocabulary mapping\n",
    "        self.vocab = {\n",
    "            self.eos_token: 0,\n",
    "            self.pad_token: 1,  # Same ID as EOS_TOKEN\n",
    "            self.unk_token: 2,\n",
    "        }\n",
    "\n",
    "        for idx, char in enumerate(characters, start=3):\n",
    "            self.vocab[char] = idx\n",
    "\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "        self.eos_token_id = self.vocab[self.eos_token]\n",
    "        self.bos_token_id = self.vocab[self.eos_token]\n",
    "        self.pad_token_id = self.vocab[self.pad_token]\n",
    "        self.unk_token_id = self.vocab[self.unk_token]\n",
    "\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def tokenize(self, data):\n",
    "        return [char for char in data]\n",
    "\n",
    "    def encode(self, data, return_tensors=None):\n",
    "        e = [self.vocab.get(char.upper(), self.unk_token) for char in data]\n",
    "        if return_tensors == 'pt':\n",
    "            return torch.tensor(e).unsqueeze(0)\n",
    "        return e\n",
    "\n",
    "    def decode(self, data):\n",
    "        try:\n",
    "            return ''.join([self.inv_vocab.get(j) for j in data])\n",
    "        except:\n",
    "            data = data.cpu().tolist()\n",
    "            return ''.join([self.inv_vocab.get(j) for j in data])\n",
    "\n",
    "    def convert_tokens_to_ids(self, token):\n",
    "        return self.vocab[token]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6kdTvuPfmoMo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer\n",
    "if config[\"token_type\"] == \"1k\":\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(\"alexgichamba/hw4_tokenizer_1k\")\n",
    "    print(\"1k vocab tokenizer loaded\")\n",
    "elif config[\"token_type\"] == \"10k\":\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(\"alexgichamba/hw4_tokenizer_10k\")\n",
    "    print(\"10k vocab tokenizer loaded\")\n",
    "elif config[\"token_type\"] == \"char\":\n",
    "    TOKENIZER = CharTokenizer()\n",
    "    print(\"character tokenizer loaded\")\n",
    "else:\n",
    "    raise ValueError(\"Invalid token type\")\n",
    "\n",
    "\n",
    "UNK_TOKEN = TOKENIZER.unk_token_id\n",
    "EOS_TOKEN = TOKENIZER.eos_token_id\n",
    "SOS_TOKEN = TOKENIZER.bos_token_id\n",
    "PAD_TOKEN = TOKENIZER.convert_tokens_to_ids('<|padding|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "csVEvmoc3pAI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281241\n",
      "2433\n"
     ]
    }
   ],
   "source": [
    "train_transcripts = [np.array([i for i in row['transcripts'].replace(\"<sos>\", \"\").replace(\"<eos>\", \"\") ]) for index, row in df_train.iterrows()]\n",
    "train_dataset = []\n",
    "for files in train_transcripts:\n",
    "    tokenized = \"\".join(files)\n",
    "    tokenized = TOKENIZER.encode(tokenized)\n",
    "    train_dataset.append(tokenized)\n",
    "\n",
    "val_transcripts = [np.array([i for i in row['transcripts'].replace(\"<sos>\", \"\").replace(\"<eos>\", \"\") ]) for index, row in df_val.iterrows()]\n",
    "val_dataset = []\n",
    "for files in val_transcripts:\n",
    "    tokenized = \"\".join(files)\n",
    "    tokenized = TOKENIZER.encode(tokenized)\n",
    "    val_dataset.append(tokenized)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "umQYdUCjmoMo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'E', 'L', 'L', 'O', ' ', 'D', 'E', 'E', 'P', ' ', 'L', 'E', 'A', 'R', 'N', 'E', 'R', 'S']\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = TOKENIZER.vocab_size\n",
    "\n",
    "# test the tokenizer\n",
    "if TOKENIZER is not None:\n",
    "    TOKENIZER.decode([EOS_TOKEN, SOS_TOKEN, PAD_TOKEN, UNK_TOKEN])\n",
    "    print(TOKENIZER.tokenize(\"HELLO DEEP LEARNERS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHjYhXAOzkrP"
   },
   "source": [
    "# Custom DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gH_fPT-fYdy1"
   },
   "outputs": [],
   "source": [
    "class DataLoaderForLanguageModeling(torch.utils.data.DataLoader): # Inherit from torch.utils.data.DataLoader\n",
    "    \"\"\"\n",
    "        TODO: Define data loader logic here\n",
    "    \"\"\"\n",
    "    # TODO: You can probably add more parameters as well. Eg. sequence length\n",
    "    def __init__(self, dataset, batch_size, sequence_length=3, shuffle= True, drop_last= False):\n",
    "\n",
    "        # If you remember, these are the standard things which you give while defining a dataloader.\n",
    "        # Now you are just customizing your dataloader\n",
    "        self.dataset    = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle    = shuffle\n",
    "        self.drop_last  = drop_last\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # What output do you get when you print len(loader)? You get the number of batches\n",
    "        # Your dataset has (579, ) articles and each article has a specified amount of words.\n",
    "        # You concatenate the dataset and then batch parts of it according to the sequence length\n",
    "        # TODO: return the number of batches\n",
    "        # If you are using variable sequence_length, the length might not be fixed\n",
    "        pass\n",
    "\n",
    "    def __iter__(self):\n",
    "        # TODO: Shuffle data if shuffle is True\n",
    "        if self.shuffle:\n",
    "            pass\n",
    "\n",
    "        # TODO: Concatenate articles drop extra words that won't fit into a full batch\n",
    "        self.concatenated_dataset = NotImplemented\n",
    "        self.num_batches = NotImplemented\n",
    "\n",
    "        if self.drop_last:\n",
    "            pass\n",
    "        else:\n",
    "            # Pad the last target sequence with EOS_TOKEN to ensure it has the same length as the other target sequences\n",
    "            pass\n",
    "\n",
    "        # TODO: Divide the concetenated dataset into inputs and targets. How do they vary?\n",
    "\n",
    "        # TODO: Reshape the inputs and targets into batches (think about the final shape)\n",
    "\n",
    "        # TODO: Loop though the batches and yield the input and target according to the sequence length\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7IyJOYqYdy1"
   },
   "outputs": [],
   "source": [
    "dl = DataLoaderForLanguageModeling(\n",
    "    dataset = NotImplemented,\n",
    "    batch_size = NotImplemented,\n",
    "    shuffle = NotImplemented,\n",
    "    drop_last = NotImplemented,\n",
    "    seq_len = NotImplemented\n",
    "    # Input Extra parameters here if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBZSzmy10M9M"
   },
   "outputs": [],
   "source": [
    "# Some sanity checks\n",
    "\n",
    "inputs, targets = next(iter(dl))\n",
    "print(inputs.shape, targets.shape)\n",
    "for x, y in dl:\n",
    "    transcript = TOKENIZER.decode(x[0].tolist())\n",
    "    transcript_y = TOKENIZER.decode(y[0].tolist())\n",
    "    print(\"x: \", transcript)\n",
    "    print(\"y: \", transcript_y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcWU0YlnzmVM"
   },
   "source": [
    "# Causal Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7wwkDXlV3xf"
   },
   "source": [
    "Causal language models predict the probability of a word based on the preceding words in the sentence. This differs from bidirectional models, which consider both previous and following context. Here, we use a Transformer-based decoder, leveraging its attention mechanism to focus only on earlier parts of the sequence to predict the next word. This type of modeling is suitable for tasks such as text generation where the sequence order is crucial.\n",
    "\n",
    "\n",
    "**Link to HuggingFace Documentation**: [Causal Language Model](https://huggingface.co/docs/transformers/en/tasks/language_modeling)\n",
    "\n",
    "The following image can be a helpful aid in visualizing the flow of information in a causal language model, highlighting how each word in a sequence is used to predict the next word.\n",
    "\n",
    "<img src=\"https://github.com/christianversloot/machine-learning-articles/blob/main/images/causal-1024x445.png?raw=true\" width=\"60%\">\n",
    "\n",
    "This figure shows three matrices: the attention scores between sequence elements, the causal mask with zeros allowing attention and negative infinity blocking future attention, and the resultant matrix after applying the causal mask. The negative infinity values in the causal mask prevent the model from using future tokens in its predictions, reinforcing the sequence's order. This visualization shows how transformers can be used for causal language modeling where future input information must not influence current predictions.\n",
    "\n",
    "<img src=\"https://github.com/christianversloot/machine-learning-articles/raw/main/images/Diagram-20-1024x282.png\" width=\"80%\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dps6lGqiYdy1"
   },
   "source": [
    "## 2. `create_mask`: Mask for Preventing Attention to Subsequent Positions\n",
    "\n",
    "\n",
    "```python\n",
    "def create_mask(seq, pad_idx=None)\n",
    "```\n",
    "\n",
    "\n",
    "## Purpose:\n",
    "This function creates a **subsequent mask** that prevents attention from attending to future positions in the sequence. It ensures that each position can only attend to previous positions (as in causal language modeling).\n",
    "\n",
    "## Usage:\n",
    "- **Input:**\n",
    "  - `seq`: Tensor of shape `(batch_size, sequence_length)` representing the input sequence.\n",
    "  - `pad_idx`: (Optional) Padding index for masking padding positions.\n",
    "\n",
    "- **Output:**\n",
    "  - A mask of shape `(batch_size, sequence_length, sequence_length)` where the upper triangular portion is filled with 1s to prevent attention to future positions.\n",
    "\n",
    "### The Expected mask should look like the image below:\n",
    "\n",
    "<img src=\"https://i.imgur.com/AJdqMGx.png\" alt=\"drawing\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v49yRYF7Ydy1"
   },
   "outputs": [],
   "source": [
    "def create_mask(seq, pad_idx=None):\n",
    "    \"\"\" Create a mask to prevent positions from attending to subsequent positions.\n",
    "\n",
    "    Args:\n",
    "        seq: The input sequence tensor, shape (batch_size, sequence_length).\n",
    "\n",
    "    Returns:\n",
    "        A mask tensor with shape (batch_size, sequence_length, sequence_length),\n",
    "            where positions are allowed to attend to previous positions but not to subsequent positions.\n",
    "    \"\"\"\n",
    "\n",
    "    sz_b, len_s = seq.size()\n",
    "\n",
    "    # Create an upper triangular matrix with zeros on the diagonal and below (indicating allowed positions)\n",
    "    #   and ones above the diagonal (indicating disallowed positions)\n",
    "\n",
    "    subsequent_mask = # TODO\n",
    "\n",
    "    # Expand the mask to match the batch size, resulting in a mask for each sequence in the batch.\n",
    "    mask = subsequent_mask.unsqueeze(0).expand(sz_b, -1, -1)  # b x ls x ls\n",
    "\n",
    "\n",
    "    ''' Create a mask to ignore padding positions in the key sequence during attention calculation. '''\n",
    "\n",
    "    # Expanding to fit the shape of key query attention matrix.\n",
    "    if pad_idx != None:\n",
    "        len_q = seq.size(1)\n",
    "\n",
    "          # Create a mask where padding positions in the key sequence are marked with 1.\n",
    "        padding_mask  = seq.eq(pad_idx)\n",
    "\n",
    "          # Expand the mask to match the dimensions of the key-query attention matrix.\n",
    "        padding_mask  = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # b x lq x lk\n",
    "\n",
    "\n",
    "        mask          = (padding_mask + mask).gt(0)\n",
    "\n",
    "    else:\n",
    "        mask = mask.gt(0)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dec_causal_mask         = create_mask( torch.randn(4, 10)  , pad_idx=0)\n",
    "\n",
    "# Black portions are attended to\n",
    "fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "axs.imshow(dec_causal_mask[0], cmap=\"gray\", aspect='auto')\n",
    "axs.set_title(\"Decoder Causal Self-Attn Mask\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1GobVvxYdy2"
   },
   "source": [
    "# Transformer Decoder Components\n",
    "\n",
    "We will use these components in the Transformer decoder. These include positional encoding, feed-forward networks, scaled dot-product attention, and multi-head attention. Each of these components plays a vital role in processing input sequences and computing attention in the Transformer model.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Positional Encoding (`PositionalEncoding`)**\n",
    "Transformers do not inherently capture the order of sequences, so positional encodings are used to introduce sequence order into the model.\n",
    "\n",
    "- **Purpose**: Adds information about the position of each token in the input sequence.\n",
    "- **Mechanism**: Uses a combination of sine and cosine functions of different frequencies to generate positional encodings.\n",
    "- **Parameters**:\n",
    "  - `projection_size`: The size of the input embeddings (i.e., `d_model`).\n",
    "  - `max_seq_len`: The maximum length of the input sequence (default: 1000).\n",
    "- **Output**: The input embedding enriched with positional information, which is passed through a dropout layer for regularization.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wixQHpDaYdy2"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, projection_size, max_seq_len= 1000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout                = torch.nn.Dropout(dropout)\n",
    "\n",
    "        pe              = torch.zeros(max_seq_len, projection_size)\n",
    "        position        = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term        = torch.exp(torch.arange(0, projection_size, 2).float() * (-math.log(10000.0) / projection_size))\n",
    "        pe[:, 0::2]     = torch.sin(position * div_term)\n",
    "        pe[:, 1::2]     = torch.cos(position * div_term)\n",
    "        pe              = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, :x.size(1)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUt8AjEEYdy2"
   },
   "source": [
    "\n",
    "## 2. **Feed-Forward Network (`FeedForward`)**\n",
    "The feed-forward network is a fully connected layer applied independently to each position in the sequence after the attention layers.\n",
    "\n",
    "- **Purpose**: Projects the intermediate representations to a higher-dimensional space and back to the original model dimension.\n",
    "- **Mechanism**: Consists of two linear layers with a GeLU activation function and dropout in between.\n",
    "- **Parameters**:\n",
    "  - `d_model`: The input and output dimensionality of the model.\n",
    "  - `d_ff`: The dimensionality of the hidden layer in the feed-forward network (default: 2048).\n",
    "  - `dropout`: Dropout rate applied after the GeLU activation (default: 0.1).\n",
    "- **Output**: The transformed input sequence passed through two linear transformations with non-linear activation in between.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5UzjJmRYdy2"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    ''' Projection Layer (Fully Connected Layers) '''\n",
    "\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1   = torch.nn.Linear(d_model, d_ff)\n",
    "        self.dropout    = torch.nn.Dropout(dropout)\n",
    "        self.linear_2   = torch.nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Apply the first linear layer, GeLU activation, and then dropout\n",
    "        x = self.dropout(torch.nn.functional.gelu(self.linear_1(x)))\n",
    "\n",
    "        # Apply the second linear layer to project the dimension back to d_model\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vofvMG5lYdy7"
   },
   "source": [
    "\n",
    "\n",
    "## 3. **Scaled Dot-Product Attention (`ScaledDotProductAttention`)**\n",
    "This module computes the attention score for each query-key pair in the input sequence using the scaled dot-product mechanism.\n",
    "\n",
    "- **Purpose**: To compute attention scores and generate weighted outputs based on the input query, key, and value matrices.\n",
    "- **Mechanism**:\n",
    "  - Calculates the dot product of queries and keys, scales by the square root of the dimension, and applies a softmax to generate attention weights.\n",
    "  - Uses dropout for regularization.\n",
    "- **Parameters**:\n",
    "  - `temperature`: Scaling factor for the dot product.\n",
    "  - `attn_dropout`: Dropout rate for attention weights (default: 0.1).\n",
    "- **Output**: Returns the weighted sum of the values and the attention weights.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMB9Jb45Ydy7"
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(torch.nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature    = temperature                       # Scaling factor for the dot product\n",
    "        self.dropout        = torch.nn.Dropout(attn_dropout)    # Dropout layer for attention weights\n",
    "        self.softmax        = torch.nn.Softmax(dim=-1)           # Softmax layer along the attention dimension\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        # Calculate the dot product between queries and keys.\n",
    "        # attn = torch.bmm(q, k.transpose(1, 2))\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        # Scale the dot product by the temperature.\n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            # Apply the mask by setting masked positions to a large negative value.\n",
    "            # This ensures they have a softmax score close to zero.\n",
    "            attn = attn.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        # Apply softmax to obtain attention weights.\n",
    "        attn    = self.softmax(attn)\n",
    "\n",
    "        # Apply dropout to the attention weights.\n",
    "        # Compute the weighted sum of values based on the attention weights.\n",
    "        # output  = torch.bmm(self.dropout(attn), v)\n",
    "        attn = self.dropout(attn)\n",
    "        output = attn @ v\n",
    "\n",
    "        return output, attn # Return the attention output and the attention weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjSRj_hjYdy7"
   },
   "source": [
    "## 4. **Multi-Head Attention (`MultiHeadAttention`)**\n",
    "This module implements multi-head attention, where multiple sets of attention heads are computed in parallel, and their outputs are concatenated.\n",
    "\n",
    "- **Purpose**: To allow the model to jointly attend to different positions in the input sequence from different representation subspaces.\n",
    "- **Mechanism**:\n",
    "  - Projects the input query, key, and value matrices into multiple smaller subspaces (heads).\n",
    "  - Computes scaled dot-product attention for each head in parallel.\n",
    "  - Concatenates the outputs of all heads and applies a final linear transformation to project the result back to the original model dimension.\n",
    "- **Parameters**:\n",
    "  - `n_head`: Number of attention heads.\n",
    "  - `d_model`: Dimensionality of the input and output representations.\n",
    "  - `dropout`: Dropout rate applied to the attention output (default: 0.1).\n",
    "- **Output**: Returns the concatenated output of all attention heads and the averaged attention weights.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRa-PPXrYdy7"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    ''' Multi-Head Attention Module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head # Number of attention heads\n",
    "        self.d_k    = d_model // n_head\n",
    "        self.d_v    = d_model // n_head\n",
    "\n",
    "\n",
    "        # Linear layers for projecting the input query, key, and value to multiple heads\n",
    "        self.w_qs   = torch.nn.Linear(d_model, n_head * self.d_k)\n",
    "        self.w_ks   = torch.nn.Linear(d_model, n_head * self.d_k)\n",
    "        self.w_vs   = torch.nn.Linear(d_model, n_head * self.d_v)\n",
    "\n",
    "        torch.nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + self.d_k)))\n",
    "        torch.nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + self.d_k)))\n",
    "        torch.nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + self.d_v)))\n",
    "\n",
    "        # Initialize the weights of the linear layers\n",
    "        self.attention = ScaledDotProductAttention(\n",
    "            temperature=np.power(self.d_k, 0.5), attn_dropout=dropout)\n",
    "\n",
    "        # Final linear layer to project the concatenated outputs of the attention heads back to the model dimension\n",
    "        self.fc = torch.nn.Linear(n_head * self.d_v, d_model)\n",
    "        torch.nn.init.normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        # following key, value, query standard computation\n",
    "        d_k, d_v, n_head    = self.d_k, self.d_v, self.n_head\n",
    "        sz_b, len_q, _      = q.size()\n",
    "        sz_b, len_k, _      = k.size()\n",
    "        sz_b, len_v, _      = v.size()\n",
    "\n",
    "        # Project the input query, key, and value to multiple heads\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        # Rearrange the dimensions to group the heads together for parallel processing\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "\n",
    "        # Repeat the mask for each attention head if a mask is provided\n",
    "        if mask is not None:\n",
    "              # print(mask.shape)\n",
    "              mask = mask.unsqueeze(1).repeat(1, n_head, 1, 1)\n",
    "\n",
    "        # Apply scaled dot-product attention to the projected query, key, and value\n",
    "        output, attn    = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # Rearrange the output back to the original order and concatenate the heads\n",
    "        output = output.transpose(1, 2).contiguous().view(sz_b, len_v, -1)\n",
    "\n",
    "        output          = self.dropout(self.fc(output))\n",
    "\n",
    "        attn_weights = attn.mean(dim=(0, 1))\n",
    "\n",
    "        return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rlvl41QtYdy7"
   },
   "source": [
    "# Transformer Decoder Layers\n",
    "\n",
    "The `DecoderLayer1` and `DecoderLayer3` are modular components of the Transformer decoder. Each layer is designed to handle a specific function: self-attention, cross-attention, and feed-forward processing.\n",
    "\n",
    "## 1. `DecoderLayer1`: Self-Attention Layer\n",
    "- **Purpose**: Implements self-attention, where the decoder attends to its own inputs, combined with residual connections and layer normalization.\n",
    "- **Components**:\n",
    "  - `MultiHeadAttention`: Applies self-attention to the target sequence.\n",
    "  - `LayerNorm`: Normalizes the output after the residual connection.\n",
    "  - `Dropout`: Regularization to prevent overfitting.\n",
    "\n",
    "## 3. `DecoderLayer3`: Feed-Forward Layer\n",
    "- **Purpose**: Implements a feed-forward neural network for further transformation of the decoder's intermediate representations.\n",
    "- **Components**:\n",
    "  - `FeedForward`: A two-layer fully connected network with non-linearity.\n",
    "  - `LayerNorm`: Applied after the residual connection.\n",
    "  - `Dropout`: Regularization to avoid overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hX2kBrAYdy7"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer1(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        DecoderLayer (attention and layer norm) in the Transformer architecture.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The number of expected features in the input (embedding dimension).\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Dimension of the feedforward network model.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(DecoderLayer1, self).__init__()\n",
    "\n",
    "        # TODO: fill in the blanks appropriately (given the modules above)\n",
    "        self.self_attn = NotImplemented\n",
    "        self.layer_norm = NotImplemented\n",
    "        self.dropout = NotImplemented\n",
    "\n",
    "    def forward(self, tgt, attn_mask=None, key_padding_mask=None):\n",
    "        # TODO: apply layer norm to input\n",
    "\n",
    "        # TODO: call self attention with mask\n",
    "\n",
    "        # TODO: apply dropout\n",
    "\n",
    "        # TODO: add skip connection\n",
    "\n",
    "        raise NotImplemented\n",
    "        # return tgt, attn_weights\n",
    "\n",
    "\n",
    "class DecoderLayer3(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        Feedforward layer with layer normalization in the Transformer decoder.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Embedding dimension.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Dimension of the feedforward network.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(DecoderLayer3, self).__init__()\n",
    "\n",
    "        # TODO: fill in the blanks appropriately (given the modules above)\n",
    "        self.ffn = NotImplemented\n",
    "        self.layer_norm = NotImplemented\n",
    "        self.dropout = NotImplemented\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        # TODO: apply layer norm to input\n",
    "\n",
    "        # TODO: call feed forward layer\n",
    "\n",
    "        # TODO: apply dropout\n",
    "\n",
    "        # TODO: add skip connection\n",
    "\n",
    "        raise NotImplemented\n",
    "        # return tgt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnMbHHLcYdy7"
   },
   "source": [
    "# Causal Language Model\n",
    "\n",
    "This module implements a Transformer-based decoder for causal language modeling (CLM). It consists of several components, including embedding layers, positional encoding, self-attention layers, and feed-forward layers. It supports various generation strategies such as beam search and sampling.\n",
    "\n",
    "### Key Components:\n",
    "- **Embedding Layer**: Converts input tokens into dense vector representations.\n",
    "- **Positional Encoding**: Adds position information to input tokens, helping the model understand the order of tokens.\n",
    "- **Decoder Layers**: Composed of:\n",
    "  - `DecoderLayer1`: Implements self-attention and layer normalization.\n",
    "  - `DecoderLayer3`: Implements a feed-forward network with residual connections.\n",
    "- **Output Linear Layer**: Projects the hidden states to the vocabulary size to generate output probabilities.\n",
    "\n",
    "### Key Methods:\n",
    "- **`forward`**: Runs the input through the decoder layers and generates output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cebwoorWttWe"
   },
   "outputs": [],
   "source": [
    "class CausalLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size=31, d_model=256, num_layers=2, num_heads=2, d_ff=512, dropout=0.1, max_length=1000):\n",
    "\n",
    "        \"\"\"\n",
    "        Decoder module in the Transformer architecture.\n",
    "        Initializes embeddings, multiple decoder layers, and an output linear layer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            d_model (int): The number of expected features in the input (embedding dimension).\n",
    "            num_layers (int): Number of decoder layers.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Dimension of the feedforward network model.\n",
    "            dropout (float): Dropout probability.\n",
    "            max_length (int): Maximum length of input sequences.\n",
    "        \"\"\"\n",
    "        super(CausalLanguageModel, self).__init__()\n",
    "\n",
    "        # TODO: fill in the blanks appropriately (given the modules above)\n",
    "\n",
    "        self.embedding = NotImplemented\n",
    "\n",
    "        self.pos_encoder = NotImplemented\n",
    "        self.num_layers= NotImplemented\n",
    "        self.dec_layers1 = NotImplemented\n",
    "        self.dec_layers3 = NotImplemented\n",
    "\n",
    "        self.layer_nrom = NotImplemented\n",
    "        self.fully_connected = NotImplemented\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # TODO: generate the causal mask using the given function\n",
    "        attn_mask = NotImplemented\n",
    "\n",
    "        # TODO: convert input to embeddings\n",
    "\n",
    "        # TODO: apply positional encoding\n",
    "\n",
    "        attention_weights_list = []\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # TODO: apply decoder layer\n",
    "            inp, attn_weights = NotImplemented\n",
    "            inp = NotImplemented\n",
    "            attention_weights_list.append(attn_weights)\n",
    "\n",
    "        # TODO: apply layernorm and the fully connected layer for classification\n",
    "        output = NotImplemented\n",
    "\n",
    "        stacked_attention_weights = torch.stack(attention_weights_list, dim=0)\n",
    "\n",
    "        return output, stacked_attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DelhoytAQWQa"
   },
   "source": [
    "# Model, Loss, Optimizer, and Scheduler Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbHH6zXTSwRa"
   },
   "outputs": [],
   "source": [
    "# TODO: Define the model\n",
    "model = CausalLanguageModel(\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    d_model    = None,\n",
    "    num_layers = None,\n",
    "    num_heads  = None,\n",
    "    d_ff       = None,\n",
    "    dropout    = None,\n",
    "    max_length = None\n",
    ").to(DEVICE)\n",
    "\n",
    "# TODO: Define the dataloader\n",
    "train_loader = DataLoaderForLanguageModeling(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=None,\n",
    "    sequence_length=None,\n",
    "    shuffle=None,\n",
    "    drop_last=None\n",
    ")\n",
    "\n",
    "val_loader = DataLoaderForLanguageModeling(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=None,\n",
    "    sequence_length=None,\n",
    "    shuffle=None,\n",
    "    drop_last=None\n",
    ")\n",
    "\n",
    "# TODO: Define the criterion\n",
    "criterion = None\n",
    "\n",
    "# TODO: Define the optimizer\n",
    "optimizer = None\n",
    "\n",
    "# TODO: Define the learning rate scheduler\n",
    "scheduler = None\n",
    "\n",
    "# Optional TODO: Define the scaler for mixed precision training\n",
    "scaler = None\n",
    "\n",
    "# Print the model architecture and parameter summary\n",
    "print(model)\n",
    "\n",
    "# Optionally, if you want to summarize the model, make sure `torchsummaryX` is installed\n",
    "summary = torchsummaryX.summary(model.to(DEVICE), x=torch.tensor(inputs).to(DEVICE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlWF_bpLznup"
   },
   "source": [
    "\n",
    "\n",
    "# Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jkFMwb095P5w"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, optimizer, criterion, scheduler, scaler, max_epochs= 1):\n",
    "        \"\"\"\n",
    "            Use this class to train your model\n",
    "        \"\"\"\n",
    "        # feel free to add any other parameters here\n",
    "        self.model      = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader   = val_loader\n",
    "        self.optimizer  = optimizer\n",
    "        self.criterion  = criterion\n",
    "        self.scheduler  = scheduler\n",
    "        self.scaler     = scaler\n",
    "\n",
    "        self.train_losses           = []\n",
    "        self.val_losses             = []\n",
    "        self.prediction_probs       = []\n",
    "        self.prediction_probs_test  = []\n",
    "        self.generated_texts_test   = []\n",
    "        self.generated_texts_test_beam = []\n",
    "        self.generated_texts_test_beam_random = []\n",
    "        self.generated_texts_validation = []\n",
    "\n",
    "        self.log_likelihood_beam = []\n",
    "        self.log_likelihood_beam_random = []\n",
    "\n",
    "        self.epochs                 = 0\n",
    "        self.max_epochs             = max_epochs\n",
    "\n",
    "\n",
    "    def calculate_loss(self, out, target):\n",
    "        # output: (B, T, Vocab_size) - probability distributions\n",
    "        # target: (B, T)\n",
    "        # Read the documentation of CrossEntropyLoss and try to understand how it takes inputs\n",
    "\n",
    "        # Tip: If your target is of shape (B, T) it means that you have B batches with T words.\n",
    "        # Tip: What is the total number of words in this batch?\n",
    "        # Tip: Crossentropy calculates the loss between a label and its probability distribution.\n",
    "\n",
    "        out     = None # TODO\n",
    "        targets = None # TODO\n",
    "        loss    = self.criterion(out, targets)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss  = 0\n",
    "        num_batches = 0\n",
    "        attn_weights = None\n",
    "\n",
    "        for batch_num, (inputs, targets) in enumerate(tqdm(self.train_loader)):\n",
    "\n",
    "            # TODO: Complete the loop. You should be able to complete this without any helper comments after 3 HWs\n",
    "            # Tip: Use Mixed Precision Training\n",
    "            # For loss calculation, use the calculate_loss function. You need to complete it before using.\n",
    "            pass\n",
    "\n",
    "\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        self.epochs += 1\n",
    "        print('[TRAIN] \\tEpoch [%d/%d] \\tLoss: %.4f \\tLr: %.6f'\n",
    "                     % (self.epochs, self.max_epochs, epoch_loss, self.optimizer.param_groups[0]['lr']))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "        return (epoch_loss, self.optimizer.param_groups[0]['lr'], attn_weights)\n",
    "\n",
    "    def validate(self):\n",
    "\n",
    "        self.model.eval() # set to eval mode\n",
    "        epoch_loss  = 0\n",
    "        num_batches = 0\n",
    "        attn_weights = None\n",
    "\n",
    "        for batch_num, (inputs, targets) in enumerate(tqdm(self.val_loader)):\n",
    "\n",
    "            # TODO: Complete the loop. You should be able to complete this without any helper comments after 3 HWs\n",
    "            # Tip: you don't need gradients for inference\n",
    "            # For loss calculation, use the calculate_loss function. You need to complete it before using it.\n",
    "            pass\n",
    "\n",
    "\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        self.epochs += 1\n",
    "        print('[VAL] \\tEpoch [%d/%d] \\tLoss: %.4f \\tLr: %.6f'\n",
    "                     % (self.epochs, self.max_epochs, epoch_loss, self.optimizer.param_groups[0]['lr']))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "        return epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HCVG5YISwRW"
   },
   "outputs": [],
   "source": [
    "# The object of the Trainer class takes in everything\n",
    "trainer = Trainer(\n",
    "    model       = model,\n",
    "    train_loader = train_loader,\n",
    "    val_loader  = val_loader,\n",
    "    optimizer   = optimizer,\n",
    "    criterion   = criterion,\n",
    "    scheduler   = scheduler,\n",
    "    scaler      = scaler,\n",
    "    max_epochs  = config[\"num_epochs\"], # TODO: set the number of epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dfrf1FoSoAI0"
   },
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91s_RQaQYdy8"
   },
   "outputs": [],
   "source": [
    "# Use wandb? Resume Training?\n",
    "USE_WANDB = True\n",
    "RESUME_LOGGING = False\n",
    "\n",
    "# Create your wandb run\n",
    "\n",
    "run_name = 'Run1' # TODO: pick a run name you like\n",
    "\n",
    "if USE_WANDB:\n",
    "\n",
    "    wandb.login(key=NotImplemented) # your wandb key\n",
    "\n",
    "    if RESUME_LOGGING:\n",
    "        run_id = ''\n",
    "        run = wandb.init(\n",
    "            settings=wandb.Settings(symlink=False),\n",
    "            id     = run_id, ### Insert specific run id here if you want to resume a previous run\n",
    "            resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "            project = \"hw4p1-f24\", ### Project should be created in your wandb account\n",
    "        )\n",
    "    else:\n",
    "        run = wandb.init(\n",
    "            name    = run_name, ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
    "            reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "            project = \"hw4p1-f24\", ### Project should be created in your wandb account\n",
    "            config  = config ### Wandb Config for your run\n",
    "        )\n",
    "\n",
    "        ### Save your model architecture as a string with str(model)\n",
    "        model_arch  = str(model)\n",
    "        ### Save it in a txt file\n",
    "        arch_file   = open(\"model_arch.txt\", \"w\")\n",
    "        file_write  = arch_file.write(model_arch)\n",
    "        arch_file.close()\n",
    "\n",
    "        ### log it in your wandb run with wandb.save()\n",
    "        # wandb.save('model_arch.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fcxKXL0hrxX"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pLh5_LCpVxw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the experiments loop.\n",
    "# Each epoch wont take more than 2-3min. If its taking more time, it might be due to (but not limited to) the following:\n",
    "#   * You might be overlapping batches\n",
    "#       Eg. Input: \"I had biryani for lunch today\" and sequence length = 3,\n",
    "#           --> \"I had biryani\", \"for lunch today\" are ideal examples for inputs\n",
    "#           --> \"I had biryani\", \"had biryani for\", \"biryani for lunch\", ... is just redundant info :')\n",
    "#   * Your length calculation in the dataloader might be wrong\n",
    "# If you haven't had biryani, try it :D\n",
    "\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    train_loss, curr_lr,  attn_weights = trainer.train()\n",
    "    print(attn_weights[-1].shape)\n",
    "\n",
    "    test_loss = trainer.validate()\n",
    "\n",
    "    wandb.log({\"train_loss\":train_loss,\n",
    "               \"test_loss\": test_loss,\n",
    "               \"learning_rate\": curr_lr\n",
    "              })\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "### Finish your wandb run\n",
    "run.finish()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EScl6rumoMv"
   },
   "source": [
    "# Testing Your Model's Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7-MtnkWiHcj"
   },
   "outputs": [],
   "source": [
    "def log_softmax(x, axis):\n",
    "    ret = x - np.max(x, axis=axis, keepdims=True)\n",
    "    lsm = np.log(np.sum(np.exp(ret), axis=axis, keepdims=True))\n",
    "    return ret - lsm\n",
    "\n",
    "def get_prediction_nll_single_for_test(out, targ):\n",
    "    out = log_softmax(out, 1)\n",
    "    nlls = out[np.arange(out.shape[0]), targ]\n",
    "    nll = -np.sum(nlls)\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWrNI6Ny2cyQ"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"dataset/test-clean/transcripts.csv\")\n",
    "test_transcripts  = []\n",
    "test_transcripts = [np.array([i for i in row['transcripts'].replace(\"<sos>\", \"\").replace(\"<eos>\", \"\") ]) for index, row in test_df.iterrows()]\n",
    "\n",
    "test_dataset = []\n",
    "for files in test_transcripts:\n",
    "    tokenized = \"\".join(files)\n",
    "    tokenized = TOKENIZER.encode(tokenized)\n",
    "    test_dataset.append(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-tmR17aiL63"
   },
   "outputs": [],
   "source": [
    "test_dl = DataLoaderForLanguageModeling(\n",
    "    dataset         = test_dataset,\n",
    "    batch_size      = 1,\n",
    "    shuffle         = False,\n",
    "    drop_last       = False,\n",
    "    sequence_length = 100\n",
    ")\n",
    "\n",
    "nnls = []\n",
    "model.eval()\n",
    "\n",
    "for batch_num, (inputs, targets) in enumerate(tqdm(test_dl)):\n",
    "    inputs = torch.tensor(inputs).long().to(DEVICE)\n",
    "    targets = torch.tensor(targets).long().to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(inputs)\n",
    "    nnl = get_prediction_nll_single_for_test(output[0][0].to('cpu').numpy(), targets[0].to('cpu').numpy())\n",
    "    if TOKENIZER != None:\n",
    "        text_len = len(TOKENIZER.decode(targets.flatten().to('cpu')).replace(\"<|endoftext|>\", \"\")) + 1\n",
    "    else:\n",
    "        text_len = len(targets[0])\n",
    "    nnls.append(nnl / text_len)\n",
    "\n",
    "test_ppl = np.exp(sum(nnls) / len(nnls))\n",
    "print(f'test_perplexity: {test_ppl}')\n",
    "with open('./hw4/test_perplexity.txt', 'w') as f:\n",
    "    f.write(str(test_ppl))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "989a3c3836794109ac641230122845a3",
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
